# -*- python -*-
# pyLRp grammar file for pyLRp
# this must be kept parsable with the limited bootstrap parser
# it is the definite reference on the format

import pyLRp
import logging

from ..syntax import Syntax, SyntaxNameError
from ..regex import Regex, RegexSyntaxError
from ..lexactions import (List, Debug, Push, Pop, Function, Token,
                          Begin, Restart, Continue)
from ..lexer import LexingRule
from ..lr import Production
from ..pyblob import PyBlobStackVarMapVisitor, PySuite, PyText, PyNewline, PyStackvar

%lexer

%x INDENT

# states used for correct inline python handling
%x PYBLOB, PYBLOBNOSPC, MLSTRD, MLSTRS, PYINLINE, PYBLOBSINGLE

# states used by the autoaction spec
%x AST

# states used by the lexer spec
%x LEXER, CONDSPEC, REGEX, DEF, DEFINDENT, LEXACTION, SCOPE, SCOPEDEF, DEFNEWLINE

# states used by the parser spec
%x PARSER, FIXITYSPEC, PARSERMETA, PARSERULE, PARSERINDENT

# states used by the footer
%x FOOTER

# symbol list states
%x SYMBOLLIST

# match the empty string as empty indentation
%nullmatch INDENT, PYINLINE

%def
    space     [\ \t\v\f]
    regexchar [^\ \t\v\f\n\r]
    regex     ({regexchar}|\\\ )+
    tonewline [^\n]*
    name      [a-zA-Z_][a-zA-Z0-9_]*
    comment   {space}*(\#{tonewline})?
    stoken    "([^\"]|\\\")+"|'([^\']|\\\')+'

#######################################################
# section switching and uniform empty line ignorance
#######################################################
<$SOL,LEXER>%ast{comment}\n    %begin(AST), AST
<$SOL,AST>%lexer{comment}\n    %begin(LEXER), LEXER
<$SOL,LEXER,AST>%parser{comment}\n %begin(PARSER), PARSER
<LEXER,AST,PARSER>%footer{comment}\n %begin(FOOTER), FOOTER

<LEXER,PARSER,AST,INDENT>{comment}\n  %restart
<INDENT>[\ \t]* %function(indent), %restart

# two never matching rules to define terminal symbols
# generated programatically (by the indent function in %footer)
[] INDENT
[] DEDENT

#######################################################
# header and footer: PYVERB
#######################################################
<FOOTER,$INITIAL>[^\n]*\n PYVERB

#######################################################
# AST directives
#######################################################
<AST>%list %push(SYMBOLLIST), LIST
<AST>%visitor %push(SYMBOLLIST), VISITOR

#######################################################
# commonly used syntactic contructs
#######################################################
<SYMBOLLIST,FIXITYSPEC>{space}+    %restart
<SYMBOLLIST,FIXITYSPEC>{comment}\n %pop(), NEWLINE
<SYMBOLLIST,FIXITYSPEC>\\\n        %restart
<SYMBOLLIST,FIXITYSPEC>,           COMMA
<SYMBOLLIST,FIXITYSPEC>{name}      SYMBOL

# fixityspecs allow stokens to appear in the list
<FIXITYSPEC>{stoken}               SYMBOL

#######################################################
# the lexer spec
#######################################################
<LEXER>%x %push(SYMBOLLIST), EXCLUSIVE
<LEXER>%s %push(SYMBOLLIST), INCLUSIVE
<LEXER>%nullmatch %push(SYMBOLLIST), NULLMATCH
<LEXER>%def{comment}\n %push(DEF), %push(INDENT), DEF
<LEXER,SCOPE>%scope %push(SCOPEDEF), SCOPE
<LEXER>\< %push(CONDSPEC), LANGLE
<LEXER>^  %push(LEXACTION), %push(REGEX), CARET
<LEXER>[^\^\<\%\s]({regexchar}|(\\\ ))* %push(LEXACTION), REGEX

<DEF>{name} SYMBOL
# this is the space separating the name and the newline
<DEF>{space}+ %begin(DEFNEWLINE), %push(REGEX), %restart
<DEFNEWLINE>{comment}\n %begin(DEF), %push(DEF), %push(INDENT), NEWLINE


<SCOPEDEF>{space}+ %restart
<SCOPEDEF>,        COMMA
<SCOPEDEF>{name}|$SOL|$SOF|$INITIAL SYMBOL
<SCOPEDEF>\+        PLUS
<SCOPEDEF>-        MINUS
<SCOPEDEF>\(       OPAREN
<SCOPEDEF>\)       CPAREN
<SCOPEDEF>{comment}\n %begin(SCOPE), %push(INDENT), %restart

<SCOPE>[^\%\n\s]({regexchar}|(\\\ ))* REGEX
<SCOPE>{space}+ %push(SCOPE), %push(INDENT), %push(LEXACTION), %restart

<CONDSPEC>{name}|$SOL|$SOF|$INITIAL SYMBOL
<CONDSPEC>,        COMMA
<CONDSPEC>{space}+ %restart
<CONDSPEC>\>       %begin(LEXACTION), %push(REGEX), RANGLE

<REGEX>{regex}  %pop(), REGEX

<LEXACTION>{space}+ %restart
<LEXACTION>,        COMMA
<LEXACTION>%push    PUSH
<LEXACTION>%pop     POP
<LEXACTION>%begin   BEGIN
<LEXACTION>%continue CONTINUE
<LEXACTION>%restart RESTART
<LEXACTION>%debug   DEBUG
<LEXACTION>"([^\"\n]|\\\")*" STRLIT
<LEXACTION>%function FUNCTION
<LEXACTION>{name}   SYMBOL
<LEXACTION>\(       OPAREN
<LEXACTION>\)       CPAREN
<LEXACTION>{comment}\n %pop(), NEWLINE
<LEXACTION>\\\n     %restart


#######################################################
# the grammar spec
#######################################################
<PARSER>{name}    %begin(PARSERMETA), SYMBOL
<PARSER>%left     %push(FIXITYSPEC),  LEFT
<PARSER>%right    %begin(FIXITYSPEC), RIGHT
<PARSER>%nonassoc %begin(FIXITYSPEC), NONASSOC
<PARSERMETA>:{comment} %begin(PARSER), %push(PARSERULE), %push(INDENT), COLON

<PARSERULE>{name}    SYMBOL
<PARSERULE>{stoken}  STOKEN
<PARSERULE>%empty    EMPTY
<PARSERULE>%error    ERROR
<PARSERULE>%prec     PREC
<PARSERULE>%AST      ASTACTION
<PARSERULE>\(        OPAREN
<PARSERULE>\)        CPAREN
<PARSERULE>\[        OBRACKET
<PARSERULE>\]        CBRACKET
<PARSERULE>{space}+  %restart
<PARSERULE>\\\n      %restart
<PARSERULE>\n        %push(PARSERULE), %push(INDENT), NEWLINE
<PARSERULE>:         %push(PYINLINE), COLON

<PYINLINE>{space}+ %restart
<PYINLINE>{comment}\n %begin(PYBLOB), %push(INDENT), %restart
<PYINLINE>() %begin(PYBLOBSINGLE), %restart

#######################################################
# python actions
#######################################################

# python strings, we must not alter their contents
# and must ignore leading whitespace and stuff within
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>[bu]?r?\"\"\" %push(MLSTRD), PYBLOB
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>[bu]?r?"([^\"\n]|\"|\\\n)*" PYBLOB
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>[bu]?r?\'\'\' %push(MLSTRS), PYBLOB
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>[bu]?r?'([^\']|\\\'|\\\n)*' PYBLOB

<MLSTRD>(\"|\"\"|[^\"\\]*)(\\\"|\\[^\"])? PYBLOB
<MLSTRD>\"\"\" %pop(), PYBLOB
<MLSTRS>(\'|\'\'|[^\'\\]*)(\\\'|\\[^\'])? PYBLOB
<MLSTRS>\'\'\' %pop(), PYBLOB

# this idea comes directly from the CPython lexer
# count grouping operators to tell whether the linebreak counts
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>[\(\[\{] %push(PYBLOBNOSPC), PYBLOB
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>[\)\]\}] %pop(), PYBLOB

<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>[a-zA-Z_][a-zA-Z0-9_]* PYBLOB
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>0x[0-9a-fA-F]+|0o[0-7]+|[0-9]+ PYBLOB
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>[=+\-*/\\.:,@<>%&|=^;]+ PYBLOB
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>$[0-9]+|$$ STACKVAR
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>${name} STACKVAR

# explicit line continuations
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>\\\n PYBLOB

# preserve insignificant space and comments
<PYBLOBNOSPC>\s+ PYBLOB
<PYBLOBNOSPC,PYBLOB,PYBLOBSINGLE>{comment} PYBLOB
<PYBLOB,PYBLOBSINGLE>{space}+ PYBLOB

# discern indentation, track newlines where appropriate
<PYBLOB>\n %push(PYBLOB), %push(INDENT), NEWLINE
<PYBLOBSINGLE>\n %push(INDENT), NEWLINE

%parser

# XXX which configurations do we really want
# XXX we definitely want an inlining mode along the lines
# XXX of %lexer "relpath" to separate the components
# facts:
# * lexer only is definitely useful
# * parser only is only useful if we define available tokens
#   and write a custom lexer
# * ast only is useful only when inlined

file:
    # full file
    init header ast lexer parser optfooter
    init header lexer ast parser optfooter
    # lexer and parser
    init header lexer parser optfooter
    # ast and parser
    # init header ast parser optfooter
    # parser only
    # init header parser optfooter
    # ast only
    # init header ast optfooter
    # lexer only
    init header lexer optfooter

init:
    %empty:
        self.syntax = Syntax()
        self.assocDefs = {}
        self.assocPower = 0
        self.productionNumber = 0

        self.undef = {}
        self.defined = set()

        self.lexscopes = [set()]

symbollist:
    symbollist_ NEWLINE: $$.sem = $1.sem

symbollist_:
    SYMBOL:                   $$.sem = [$1.sem]
    symbollist_ COMMA SYMBOL: $$.sem = $1.sem + [$3.sem]
    symbollist_ SYMBOL:       $$.sem = $1.sem + [$2.sem]
    symbollist_ %error COMMA: $$.sem = $1.sem

pyblob:
    pysingle NEWLINE: $$.sem = $1.sem.get()
    INDENT pyblob_ DEDENT: $$.sem = $2.sem.get()

pysingle:
    PYBLOB: $$.sem = PyBlobBuilder(); $$.sem.text($1.sem)
    STACKVAR: $$.sem = PyBlobBuilder(); $$.sem.stackvar($1.sem)
    pysingle PYBLOB: $$.sem = $1.sem; $$.sem.text($2.sem)
    pysingle STACKVAR: $$.sem = $1.sem; $$.sem.stackvar($2.sem)

pyblob_:
    PYBLOB: $$.sem = PyBlobBuilder(); $$.sem.text($1.sem)
    STACKVAR: $$.sem = PyBlobBuilder(); $$.sem.stackvar($1.sem)
    INDENT pyblob_ DEDENT: $$.sem = PyBlobBuilder(); $$.sem.suite($2.sem.get())
    pyblob_ PYBLOB: $$.sem = $1.sem; $$.sem.text($2.sem)
    pyblob_ STACKVAR: $$.sem = $1.sem; $$.sem.stackvar($2.sem)
    pyblob_ INDENT pyblob_ DEDENT: $$.sem = $1.sem; $$.sem.suite($3.sem.get())
    pyblob_ NEWLINE: $$.sem = $1.sem; $$.sem.newline()

header:
    %empty
    header PYVERB: self.syntax.add_header($2.sem)

ast:
    AST: self.syntax.AST_info.set_used()
    ast LIST symbollist:
        for name in $3.sem:
            self.syntax.AST_info.list(name)

    ast VISITOR symbollist:
       try:
           self.syntax.AST_info.visitor, = $3.sem
       except ValueError:
           # XXX signal error!
           pass

lexer:
    LEXER lexdecs lexrules

lexdecs:
    %empty
    lexdecs lexdec

lexdec:
    EXCLUSIVE symbollist:
        for symb in $2.sem:
            self.syntax.add_exclusive_initial_condition(symb)

    INCLUSIVE symbollist:
        for symb in $2.sem:
            self.syntax.add_inclusive_initial_condition(symb)

    NULLMATCH symbollist:
        for symb in $2.sem:
            self.syntax.initial_condition(symb).seclare_nullmatch()

    DEF INDENT lexdefs DEDENT

lexdefs:
    SYMBOL REGEX NEWLINE:
        self.syntax.add_named_pattern(
            $1.sem,
            Regex($2.sem, bindings=self.syntax.named_patterns()).ast
        )

    lexdefs SYMBOL REGEX NEWLINE:
        self.syntax.add_named_pattern(
            $2.sem,
            Regex($3.sem, bindings=self.syntax.named_patterns()).ast
        )

lexrules:
    %empty
    lexrules lexrule

lexrule:
    lexscope
    REGEX lexaction:
        regex = Regex($1.sem, bindings=self.syntax.named_patterns())
        self.syntax.add_lexing_rule(LexingRule(set(), regex, $2.sem))
    CARET REGEX lexaction:
        regex = Regex($2.sem, bindings=self.syntax.named_patterns())
        self.syntax.add_lexing_rule(LexingRule(set([self.syntax.initial_condition("$SOL")]), regex, $3.sem))

    LANGLE condspec RANGLE REGEX lexaction:
        regex = Regex($4.sem, bindings=self.syntax.named_patterns())
        state = set()
        for symb in $2.sem:
            state.add(self.syntax.initial_condition(symb))
        self.syntax.add_lexing_rule(LexingRule(state, regex, $5.sem))

lexscope:
    lexscopedecl INDENT lexscope_ DEDENT: self.lexscopes.pop()

lexscopedecl:
    SCOPE OPAREN scopespec CPAREN:
        state = set(self.lexscopes[-1])
        for op, cond in $3.sem:
            cond = self.syntax.initial_condition(cond)
            if op == '+':
                state.add(cond)
            elif op == '-':
                state.discard(cond)
        self.lexscopes.append(state)

scopespec:
    SYMBOL: $$.sem = [('+', $1.sem)]
    MINUS SYMBOL: $$.sem = [('-', $2.sem)]
    PLUS SYMBOL: $$.sem = [('+', $2.sem)]
    scopespec COMMA SYMBOL: $$.sem = $1.sem + [('+', $3.sem)]
    scopespec COMMA MINUS SYMBOL: $$.sem = $1.sem + [('-', $3.sem)]
    scopespec COMMA PLUS SYMBOL: $$.sem = $1.sem + [('+', $3.sem)]

lexscope_:
    lexscope
    REGEX lexaction:
       regex = Regex($1.sem, bindings=self.syntax.named_patterns())
       self.syntax.add_lexing_rule(LexingRule(self.lexscopes[-1], regex, $2.sem))
    lexscope_ lexscope
    lexscope_ REGEX lexaction:
       regex = Regex($2.sem, bindings=self.syntax.named_patterns())
       self.syntax.add_lexing_rule(LexingRule(self.lexscopes[-1], regex, $3.sem))

condspec:
    SYMBOL: $$.sem = [$1.sem]
    condspec COMMA SYMBOL: $$.sem = $1.sem + [$3.sem]

lexaction:
    stackactions final NEWLINE: $$.sem = $1.sem; $$.sem.append($2.sem)

stackactions:
    %empty: $$.sem = List()
    stackactions PUSH OPAREN SYMBOL CPAREN COMMA:
        $$.sem = $1.sem
        $$.sem.append(Push(self.syntax.initial_condition($4.sem)))
    stackactions POP OPAREN CPAREN COMMA:
        $$.sem = $1.sem
        $$.sem.append(Pop())
    stackactions BEGIN OPAREN SYMBOL CPAREN COMMA:
        $$.sem = $1.sem
        $$.sem.append(Begin(self.syntax.initial_condition($4.sem)))
    stackactions FUNCTION OPAREN SYMBOL CPAREN COMMA:
        $$.sem = $1.sem
        $$.sem.append(Function($4.sem))
    stackactions DEBUG OPAREN STRLIT CPAREN COMMA:
        $$.sem = $1.sem
        $$.sem.append(Debug($4.sem))

final:
    RESTART: $$.sem = Restart()
    CONTINUE: $$.sem = Continue()
    SYMBOL:
        self.syntax.require_terminal($1.sem)
        $$.sem = Token($1.sem)

parser:
    PARSER assocdefs
    parser ruleset DEDENT

assocdefs:
    %empty
    assocdefs assocdef

assocdef:
    LEFT symbollist: set_assoc(self, Production.LEFT, $2.sem)
    RIGHT symbollist: set_assoc(self, Production.RIGHT, $2.sem)
    NONASSOC symbollist: set_assoc(self, Production.NONASSOC, $2.sem)

ruleset:
    SYMBOL COLON INDENT:
        symbol = self.syntax.require_meta($1.sem)

        if symbol in self.undef:
            del self.undef[symbol]
        self.defined.add(symbol)

        if self.syntax.start_symbol is None:
            self.syntax.start_symbol = symbol

        $$.sem = symbol
    ruleset parserule:
        $$.sem = $1.sem
        $$.sem.add_prod($2.sem)

parserule:
    production COLON pyblob:
        # map the stack vars to numbers
        varmapper = PyBlobStackVarMapVisitor($1.sem.varmap)
        varmapper.visit($3.sem)
        $$.sem = $1.sem.production
        $$.sem.action = $3.sem
    production NEWLINE: $$.sem = $1.sem.production
    production ASTACTION OPAREN SYMBOL CPAREN NEWLINE:
        $$.sem = $1.sem.production
        self.syntax.AST_info.bind($$.sem, $4.sem)

production:
    SYMBOL:
        $$.sem = setup_production(self)
        add_meta_symbol(self, $$.sem, $1.sem, $1.pos)

    EMPTY: $$.sem = setup_production(self)

    ERROR:
        $$.sem = setup_production(self)
        add_error(self, $$.sem,  $1.pos)

    STOKEN:
        $$.sem = setup_production(self)
        add_stoken(self, $$.sem, $1.sem, $1.pos)

    production PREC OPAREN SYMBOL CPAREN:
        $$.sem = $1.sem
        $$.sem.production.assoc = self.assocDefs[$4.sem]

    production SYMBOL:
        $$.sem = $1.sem
        add_meta_symbol(self, $$.sem, $2.sem, $2.pos)

    production SYMBOL OBRACKET SYMBOL CBRACKET:
        $$.sem = $1.sem

        add_meta_symbol(self, $$.sem, $2.sem, $2.pos, altname=$4.sem)

    production EMPTY: $$.sem = $1.sem

    production ERROR:
        $$.sem = $1.sem
        add_error(self, $$.sem,  $1.pos)

    production STOKEN:
        $$.sem = $1.sem
        add_stoken(self, $$.sem, $2.sem, $2.pos)


optfooter:
    %empty
    footer

footer:
    FOOTER
    footer PYVERB: self.syntax.add_footer($2.sem)

%footer

def indent(lexer, text, position):
    if not hasattr(lexer, 'indent_stack'):
        lexer.indent_stack = [0]

    if len(text) > lexer.indent_stack[-1]:
        lexer.indent_stack.append(len(text))
        lexer.nextCond.pop()
        lexer.push_back((lexer.TOKEN_NUMBERS['INDENT'], text, position))
        return

    # pop the state pushed on INDENT
    lexer.nextCond.pop()

    cnt = 0
    try:
        while lexer.indent_stack.pop() != len(text):
            cnt += 1
    except IndexError:
        return (lexer.TOKEN_NUMBERS['$ERROR'], text, position)

    lexer.indent_stack.append(len(text))

    for i in range(cnt):
        # pop one further state per DEDENT
        lexer.nextCond.pop()
        lexer.push_back((lexer.TOKEN_NUMBERS['DEDENT'], text, position))

    lexer.nextCond.pop()

class AnnotatedProduction(object):

    def __init__(self, production):
        """
        A `Production` annotated with names for the single elements.
        """
        self.production = production
        self.names = {}
        self.ambiguous = set()

    def varmap(self, var):
        """
        A varmap function for the PyBlobMapStackVarVisitor
        """
        text = var[1:]

        if text == '$':
            return None

        try:
            num = int(text)
        except ValueError:
            try:
                if text in self.ambiguous:
                    logging.getLogger('pyLR1').error("Ambiguous stack var")

                num = self.names[text]
            except KeyError:
                logging.getLogger('pyLR1').error("Undefined stack var")
                return ''

        return num


    def name_last(self, name, explicit=False):
        # TODO: handle ambiguity (with warnings/errors)
        # TODO: do we want the $$ renaming feature of bison
        # I personally feel, that $$ does not lead to the
        # kind of unreadability and to the difficulties of
        # $N
        if name in self.names:
            if explicit:
                logging.getLogger('pyLR1').error("Explicitely redefined symbol name")
            else:
                self.ambiguous.add(name)

        self.names[name] = len(self.production) - 1

def setup_production(self):
    prod = Production(None, [], self.productionNumber)
    self.productionNumber += 1
    return AnnotatedProduction(prod)

def set_assoc(parser, assoctype, symbols):
    assoc = assoctype, parser.assocPower
    for symb in symbols:
        parser.assocDefs[symb] = assoc
    parser.assocPower += 1

def add_meta_symbol(self, prod, symbol, pos, altname=None):
    sym = self.syntax.require_meta(symbol)
    if self.syntax[sym.name].symtype == Syntax.META \
            and sym not in self.defined:
        self.undef.setdefault(sym, []).append(pos)

    prod.production.assoc = self.assocDefs.get(sym.name,
                                               prod.production.assoc)
    prod.production.add_sym(sym)

    prod.name_last(symbol)
    if altname is not None:
        prod.name_last(altname, explicit=True)

def add_stoken(self, prod, stoken, pos):
    sym = self.syntax.require_terminal(stoken, stoken=True)
    # XXX: maybe we should do this by hand
    self.syntax.add_inline_lexing_rule(bytes(stoken[1:-1], "utf-8").decode('unicode-escape'))

    prod.production.assoc = self.assocDefs.get(sym.name,
                                               prod.production.assoc)
    prod.production.add_sym(sym)

def add_error(self, prod, pos):
    sym = self.syntax.require_terminal("$RECOVER")
    prod.production.add_sym(sym)

class PyBlobBuilder(object):
    def __init__(self):
        self.blob = PySuite()

    def get(self):
        return self.blob

    def newline(self):
        self.blob.add(PyNewline())

    def text(self, text):
        self.blob.add(PyText(text))

    def stackvar(self, sv):
        self.blob.add(PyStackvar(text=sv))

    def suite(self, suite):
        self.blob.add(suite)


# for debugging purposes
if __name__ == '__main__':
    import sys
    l = Lexer(open(sys.argv[2], 'r'))

    if sys.argv[1] == '-l':
        token_type = -1
        while token_type != Lexer.TOKEN_NUMBERS['$EOF']:
            token_type, lexeme, position = l.lex()
            print(Lexer.TOKEN_NAMES[token_type], '"""{}"""'.format(lexeme), l.nextCond)
    elif sys.argv[1] == '-p':
        p = Parser(l)
        try:
            p.Parse()
        except SyntaxError as e:
            print(str(e))
