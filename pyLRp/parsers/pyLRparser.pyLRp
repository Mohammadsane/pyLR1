# -*- python -*-
# pyLRp grammar file for pyLRp
# this must be kept parsable with the limited bootstrap parser
# it is the definite reference on the format

import pyLRp
import logging

from ..syntax import Syntax, SyntaxNameError, Symtable
from ..regex import Regex, RegexSyntaxError
from ..lexactions import (List, Debug, Push, Pop, Function, Token,
                          Begin, Restart, Continue)
from ..lexer import LexingRule
from ..lr import Production
from ..pyblob import PyBlobVisitor, PySuite, PyText, PyNewline, PyStackvar

%lexer

%x INDENT

# states used for correct inline python handling
%x PYBLOB, PYBLOBNOSPC, MLSTRD, MLSTRS, PYINLINE, PYBLOBSINGLE

# states used by the autoaction spec
%x AST

# states used by the lexer spec
%x LEXER, CONDSPEC, REGEX, DEF, DEFINDENT, LEXACTION
%x SCOPE, SCOPEDEF, DEFNEWLINE

# states used by the parser spec
%x PARSER, FIXITYSPEC, PARSERMETA, PARSERULE, PARSERINDENT

# states used by the footer
%x FOOTER

# symbol list states
%x SYMBOLLIST

# match the empty string as empty indentation
%nullmatch INDENT, PYINLINE

%def
    space     [\ \t\v\f]
    regexchar [^\ \t\v\f\n\r]
    regex     ({regexchar}|\\\ )+
    tonewline [^\n]*
    name      [a-zA-Z_][a-zA-Z0-9_]*
    comment   {space}*(\#{tonewline})?
    stoken    "([^\"]|\\\")+"|'([^\']|\\\')+'

#######################################################
# section switching and uniform empty line ignorance
#######################################################
<$SOL,LEXER>%ast{comment}\n    %begin(AST), AST
<$SOL,AST>%lexer{comment}\n    %begin(LEXER), LEXER
<$SOL,LEXER,AST>%parser{comment}\n %begin(PARSER), PARSER
<LEXER,AST,PARSER>%footer{comment}\n %begin(FOOTER), FOOTER

<LEXER,PARSER,AST,INDENT>{comment}\n  %restart
<INDENT>[\ \t]* %function(indent), %restart

# two never matching rules to define terminal symbols
# generated programatically (by the indent function in %footer)
# a %token directive might be a good idea ...
[] INDENT
[] DEDENT

#######################################################
# header and footer: PYVERB
#######################################################
<FOOTER,$INITIAL>[^\n]*\n PYVERB

#######################################################
# AST directives
#######################################################
<AST>%list %push(SYMBOLLIST), LIST
<AST>%visitor %push(SYMBOLLIST), VISITOR

#######################################################
# commonly used syntactic contructs
#######################################################
<SYMBOLLIST,FIXITYSPEC>{space}+    %restart
<SYMBOLLIST,FIXITYSPEC>{comment}\n %pop(), NEWLINE
<SYMBOLLIST,FIXITYSPEC>\\\n        %restart
<SYMBOLLIST,FIXITYSPEC>,           COMMA
<SYMBOLLIST,FIXITYSPEC>{name}      SYMBOL

# fixityspecs allow stokens to appear in the list
# they are otherwise paresed by the same grammar
# in the grammar description
<FIXITYSPEC>{stoken}               STOKEN

#######################################################
# the lexer spec
#######################################################
<LEXER>%x %push(SYMBOLLIST), EXCLUSIVE
<LEXER>%s %push(SYMBOLLIST), INCLUSIVE
<LEXER>%nullmatch %push(SYMBOLLIST), NULLMATCH
<LEXER>%def{comment}\n %push(DEF), %push(INDENT), DEF
<LEXER,SCOPE>%scope %push(SCOPEDEF), SCOPE
<LEXER>\< %push(CONDSPEC), LANGLE
<LEXER>^  %push(LEXACTION), %push(REGEX), CARET
<LEXER>[^\^\<\%\s]({regexchar}|(\\\ ))* %push(LEXACTION), REGEX

<DEF>{name} SYMBOL
# this is the space separating the name and the regex
<DEF>{space}+ %begin(DEFNEWLINE), %push(REGEX), %restart
<DEFNEWLINE>{comment}\n %begin(DEF), %push(DEF), %push(INDENT), NEWLINE


<SCOPEDEF>{space}+ %restart
<SCOPEDEF>,        COMMA
<SCOPEDEF>{name}|$SOL|$SOF|$INITIAL SYMBOL
<SCOPEDEF>\+        PLUS
<SCOPEDEF>-        MINUS
<SCOPEDEF>\(       OPAREN
<SCOPEDEF>\)       CPAREN
<SCOPEDEF>{comment}\n %begin(SCOPE), %push(INDENT), %restart

<SCOPE>[^\%\n\s]({regexchar}|(\\\ ))* REGEX
<SCOPE>{space}+ %push(SCOPE), %push(INDENT), %push(LEXACTION), %restart

<CONDSPEC>{name}|$SOL|$SOF|$INITIAL SYMBOL
<CONDSPEC>,        COMMA
<CONDSPEC>{space}+ %restart
<CONDSPEC>\>       %begin(LEXACTION), %push(REGEX), RANGLE

<REGEX>{regex}  %pop(), REGEX

<LEXACTION>{space}+ %restart
<LEXACTION>,        COMMA
<LEXACTION>%push    PUSH
<LEXACTION>%pop     POP
<LEXACTION>%begin   BEGIN
<LEXACTION>%continue CONTINUE
<LEXACTION>%restart RESTART
<LEXACTION>%debug   DEBUG
<LEXACTION>"([^\"\n]|\\\")*" STRLIT
<LEXACTION>%function FUNCTION
<LEXACTION>{name}   SYMBOL
<LEXACTION>$INITIAL INITIAL
<LEXACTION>\(       OPAREN
<LEXACTION>\)       CPAREN
<LEXACTION>{comment}\n %pop(), NEWLINE
<LEXACTION>\\\n     %restart


#######################################################
# the grammar spec
#######################################################
<PARSER>{name}    %begin(PARSERMETA), SYMBOL
<PARSER>%left     %push(FIXITYSPEC),  LEFT
<PARSER>%right    %push(FIXITYSPEC), RIGHT
<PARSER>%nonassoc %push(FIXITYSPEC), NONASSOC
<PARSERMETA>{space}+ %restart
<PARSERMETA>\[     OBRACKET
<PARSERMETA>\]     CBRACKET
<PARSERMETA>{name} SYMBOL
<PARSERMETA>:{comment} %begin(PARSER), %push(PARSERULE), %push(INDENT), COLON

<PARSERULE>{name}    SYMBOL
<PARSERULE>{stoken}  STOKEN
<PARSERULE>%empty    EMPTY
<PARSERULE>%error    ERROR
<PARSERULE>%prec     PREC
<PARSERULE>%AST      ASTACTION
<PARSERULE>\(        OPAREN
<PARSERULE>\)        CPAREN
<PARSERULE>\[        OBRACKET
<PARSERULE>\]        CBRACKET
<PARSERULE>{space}+  %restart
<PARSERULE>\\\n      %restart
<PARSERULE>\n        %push(PARSERULE), %push(INDENT), NEWLINE
<PARSERULE>:         %push(PYINLINE), COLON

<PYINLINE>{space}+ %restart
<PYINLINE>{comment}\n %begin(PYBLOB), %push(INDENT), %restart
<PYINLINE>() %begin(PYBLOBSINGLE), %restart

#######################################################
# python actions
#######################################################

# python strings, we must not alter their contents
# and must ignore leading whitespace and stuff within
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>[bu]?r?\"\"\" %push(MLSTRD), PYBLOB
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>[bu]?r?"([^\"\n]|\"|\\\n)*" PYBLOB
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>[bu]?r?\'\'\' %push(MLSTRS), PYBLOB
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>[bu]?r?'([^\'\n]|\\\'|\\\n)*' PYBLOB

<MLSTRD>(\"|\"\"|[^\"\\]*)(\\\"|\\[^\"])? PYBLOB
<MLSTRD>\"\"\" %pop(), PYBLOB
<MLSTRS>(\'|\'\'|[^\'\\]*)(\\\'|\\[^\'])? PYBLOB
<MLSTRS>\'\'\' %pop(), PYBLOB

# this idea comes directly from the CPython lexer
# count grouping operators to tell whether the linebreak counts
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>[\(\[\{] %push(PYBLOBNOSPC), PYBLOB
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>[\)\]\}] %pop(), PYBLOB

<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>[a-zA-Z_][a-zA-Z0-9_]* PYBLOB
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>0x[0-9a-fA-F]+|0o[0-7]+|[0-9]+ PYBLOB
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>[=+\-*/\\.:,@<>%&|=^;]+ PYBLOB
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>$[0-9]+|$$ STACKVAR
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>${name} STACKVAR

# explicit line continuations
<PYBLOB,PYBLOBNOSPC,PYBLOBSINGLE>\\\n PYBLOB

# preserve insignificant space and comments
<PYBLOBNOSPC>\s+ PYBLOB
<PYBLOBNOSPC,PYBLOB,PYBLOBSINGLE>{comment} PYBLOB
<PYBLOB,PYBLOBSINGLE>{space}+ PYBLOB

# discern indentation, track newlines where appropriate
<PYBLOB>\n %push(PYBLOB), %push(INDENT), NEWLINE
<PYBLOBSINGLE>\n %push(INDENT), NEWLINE

%parser

# XXX which configurations do we really want
# XXX we definitely want an inlining mode along the lines
# XXX of %lexer "relpath" to separate the components
# facts:
# * lexer only is definitely useful
# * parser only is only useful if we define available tokens
#   and write a custom lexer
# * ast only is useful only when inlined

file:
    # full file
    init header ast lexer parser optfooter
    init header lexer ast parser optfooter
    # lexer and parser
    init header lexer parser optfooter
    # ast and parser
    # init header ast parser optfooter
    # parser only
    # init header parser optfooter
    # ast only
    # init header ast optfooter
    # lexer only
    init header lexer optfooter

init:
    %empty:
        self.syntax = Syntax()
        self.assocDefs = {}
        self.assocPower = 0
        self.productionNumber = 0

        self.lexscopes = [set()]

symbollist:
    symbollist_ NEWLINE: $$.sem = $1.sem

symbollist_:
    SYMBOL:                   $$.sem = [$1.sem]
    symbollist_ COMMA SYMBOL: $$.sem = $1.sem + [$3.sem]
    symbollist_ SYMBOL:       $$.sem = $1.sem + [$2.sem]
    symbollist_ %error COMMA: $$.sem = $1.sem

    # this can only occur in fixity spec lists,
    # because only there the lexer-states allow STOKEN.
    # in fixity specs we want normalizes stoken names.
    STOKEN:
        text, normal_name = self.syntax.normalize_s_token_name($1.sem)
        $$.sem = [normal_name]
    symbollist_ STOKEN:
        text, normal_name = self.syntax.normalize_s_token_name($2.sem)
        $$.sem = $1.sem + [normal_name]

    symbollist_ COMMA STOKEN:
        text, normal_name = self.syntax.normalize_s_token_name($3.sem)
        $$.sem = $1.sem + [normal_name]


pyblob:
    pysingle NEWLINE: $$.sem = $1.sem.get()
    INDENT pyblob_ DEDENT: $$.sem = $2.sem.get()

pysingle:
    PYBLOB: $$.sem = PyBlobBuilder(); $$.sem.text($1.sem)
    STACKVAR: $$.sem = PyBlobBuilder(); $$.sem.stackvar($1.sem, $1.pos)
    pysingle PYBLOB: $$.sem = $1.sem; $$.sem.text($2.sem)
    pysingle STACKVAR: $$.sem = $1.sem; $$.sem.stackvar($2.sem, $2.pos)

pyblob_:
    PYBLOB: $$.sem = PyBlobBuilder(); $$.sem.text($1.sem)
    STACKVAR: $$.sem = PyBlobBuilder(); $$.sem.stackvar($1.sem, $1.pos)
    INDENT pyblob_ DEDENT: $$.sem = PyBlobBuilder(); $$.sem.suite($2.sem.get())
    pyblob_ PYBLOB: $$.sem = $1.sem; $$.sem.text($2.sem)
    pyblob_ STACKVAR: $$.sem = $1.sem; $$.sem.stackvar($2.sem, $2.pos)
    pyblob_ INDENT pyblob_ DEDENT: $$.sem = $1.sem; $$.sem.suite($3.sem.get())
    pyblob_ NEWLINE: $$.sem = $1.sem; $$.sem.newline()

header:
    %empty
    header PYVERB: self.syntax.header.add($2.sem)

ast:
    AST: self.syntax.AST_info.set_used()
    ast LIST symbollist:
        for name in $3.sem:
            self.syntax.AST_info.list(name)

    ast VISITOR symbollist:
       try:
           self.syntax.AST_info.visitor, = $3.sem
       except ValueError:
           # XXX signal error!
           pass

lexer:
    LEXER lexdecs lexrules

lexdecs:
    %empty
    lexdecs lexdec

lexdec:
    EXCLUSIVE symbollist:
        for symb in $2.sem:
            try:
                self.syntax.lexer.add_exclusive_initial_condition(symb)
            except SyntaxNameError as e:
                error(self, $2.pos, e.args[0])

    INCLUSIVE symbollist:
        for symb in $2.sem:
            try:
                self.syntax.lexer.add_inclusive_initial_condition(symb)
            except SyntaxNameError as e:
                error(self, $2.pos, e.args[0])

    NULLMATCH symbollist:
        for symb in $2.sem:
            get_initial_condition(self, $2.pos, symb).declare_nullmatch()

    DEF INDENT lexdefs DEDENT

lexdefs:
    SYMBOL REGEX NEWLINE: bind_named_pattern(self, $1, $2)
    lexdefs SYMBOL REGEX NEWLINE: bind_named_pattern(self, $2, $3)

lexrules:
    %empty
    lexrules lexrule

lexrule:
    lexscope
    REGEX lexaction: add_lexrule(self, set(), $1, $2)
    CARET REGEX lexaction:
        add_lexrule(self, set([get_initial_condition(self, $1.pos, "$SOL")]),
                    $2, $3)
    LANGLE condspec RANGLE REGEX lexaction: add_lexrule(self, $2.sem, $4, $5)

lexscope:
    lexscopedecl INDENT lexscope_ DEDENT: self.lexscopes.pop()

lexscopedecl:
    SCOPE OPAREN scopespec CPAREN:
        state = set(self.lexscopes[-1])
        for op, cond in $3.sem:
            cond = get_initial_condition(self, $3.pos, cond)
            if op == '+':
                state.add(cond)
            elif op == '-':
                state.discard(cond)
        if not state:
            warning($4.pos, "Defined empty start condition scope")
        self.lexscopes.append(state)

scopespec:
    SYMBOL: $$.sem = [('+', $1.sem)]
    MINUS SYMBOL: $$.sem = [('-', $2.sem)]
    PLUS SYMBOL: $$.sem = [('+', $2.sem)]
    scopespec COMMA SYMBOL: $$.sem = $1.sem + [('+', $3.sem)]
    scopespec COMMA MINUS SYMBOL: $$.sem = $1.sem + [('-', $3.sem)]
    scopespec COMMA PLUS SYMBOL: $$.sem = $1.sem + [('+', $3.sem)]

lexscope_:
    lexscope
    REGEX lexaction: add_lexrule(self, self.lexscopes[-1], $1, $2)
    lexscope_ lexscope
    lexscope_ REGEX lexaction: add_lexrule(self, self.lexscopes[-1], $2, $3)

condspec:
    SYMBOL: $$.sem = [get_initial_condition(self, $1.pos, $1.sem)]
    condspec COMMA SYMBOL:
        $$.sem = $1.sem + [get_initial_condition(self, $3.pos, $3.sem)]

lexaction:
    stackactions final NEWLINE: $$.sem = $1.sem; $$.sem.append($2.sem)

stackactions:
    %empty: $$.sem = List()
    stackactions PUSH OPAREN SYMBOL CPAREN COMMA:
        $$.sem = $1.sem
        $$.sem.append(Push(get_initial_condition(self, $4.pos, $4.sem)))
    stackactions POP OPAREN CPAREN COMMA:
        $$.sem = $1.sem
        $$.sem.append(Pop())
    stackactions BEGIN OPAREN SYMBOL CPAREN COMMA:
        $$.sem = $1.sem
        $$.sem.append(Begin(get_initial_condition(self, $4.pos, $4.sem)))
    stackactions BEGIN OPAREN INITIAL CPAREN COMMA:
        $$.sem = $1.sem
        $$.sem.append(Begin(get_initial_condition(self, $4.pos, $4.sem)))
    stackactions FUNCTION OPAREN SYMBOL CPAREN COMMA:
        $$.sem = $1.sem
        $$.sem.append(Function($4.sem))
    stackactions DEBUG OPAREN STRLIT CPAREN COMMA:
        $$.sem = $1.sem
        $$.sem.append(Debug($4.sem))

final:
    RESTART: $$.sem = Restart()
    CONTINUE: $$.sem = Continue()
    SYMBOL:
        self.syntax.symtable.define_terminal($1.sem)
        $$.sem = Token($1.sem)

parser:
    PARSER assocdefs
    parser ruleset DEDENT

assocdefs:
    %empty
    assocdefs assocdef

assocdef:
    LEFT symbollist: set_assoc(self, Production.LEFT, $2.sem)
    RIGHT symbollist: set_assoc(self, Production.RIGHT, $2.sem)
    NONASSOC symbollist: set_assoc(self, Production.NONASSOC, $2.sem)

ruleset:
    SYMBOL COLON INDENT:
        symbol = self.syntax.symtable.define_meta($1.sem)

        if self.syntax.grammar.start_symbol is None:
            self.syntax.grammar.start_symbol = symbol

        $$.sem = AnnotatedRuleset(symbol, $1.pos)
    SYMBOL OBRACKET SYMBOL CBRACKET COLON INDENT:
        symbol = self.syntax.symtable.define_meta($1.sem)

        if self.syntax.grammar.start_symbol is None:
            self.syntax.grammar.start_symbol = symbol

        $$.sem = AnnotatedRuleset(symbol, $3.pos, $3.sem)

    ruleset parserule:
        $$.sem = $1.sem
        $$.sem.add_prod($2.sem)

parserule:
    production COLON pyblob:
        $$.sem = $1.sem
        $$.sem.action = $3.sem
    production NEWLINE: $$.sem = $1.sem
    production ASTACTION OPAREN SYMBOL CPAREN NEWLINE:
        $$.sem = $1.sem
        self.syntax.AST_info.bind($$.sem.production, $4.sem)

production:
    SYMBOL:
        $$.sem = setup_production(self)
        add_meta_symbol(self, $$.sem, $1.sem, $1.pos)

    SYMBOL OBRACKET SYMBOL CBRACKET:
        $$.sem = setup_production(self)
        add_meta_symbol(self, $$.sem, $1.sem, $1.pos, altname=$3.sem)

    EMPTY: $$.sem = setup_production(self)

    ERROR:
        $$.sem = setup_production(self)
        add_error(self, $$.sem,  $1.pos)

    STOKEN:
        $$.sem = setup_production(self)
        add_stoken(self, $$.sem, $1.sem, $1.pos)

    production PREC OPAREN SYMBOL CPAREN:
        $$.sem = $1.sem
        try:
            $$.sem.production.assoc = self.assocDefs[$4.sem]
        except KeyError:
            errmsg = "precedence of symbol used in %prec undefined"
            error(self, $4.pos, errmsg)

    production PREC OPAREN STOKEN CPAREN:
        $$.sem = $1.sem
        text, normal_name = self.syntax.normalize_s_token_name($4.sem)
        try:
            $$.sem.production.assoc = self.assocDefs[normal_name]
        except KeyError:
            errmsg = "precedence of symbol used in %prec undefined"
            error(self, $4.pos, errmsg)

    production SYMBOL:
        $$.sem = $1.sem
        add_meta_symbol(self, $$.sem, $2.sem, $2.pos)

    production SYMBOL OBRACKET SYMBOL CBRACKET:
        $$.sem = $1.sem
        add_meta_symbol(self, $$.sem, $2.sem, $2.pos, altname=$4.sem)

    production EMPTY: $$.sem = $1.sem

    production ERROR:
        $$.sem = $1.sem
        add_error(self, $$.sem,  $1.pos)

    production STOKEN:
        $$.sem = $1.sem
        add_stoken(self, $$.sem, $2.sem, $2.pos)


optfooter:
    %empty
    footer

footer:
    FOOTER
    footer PYVERB: self.syntax.footer.add($2.sem)

%footer

# overwrite the default error and warning functions
def error(parser, pos, errmsg):
    logger = logging.getLogger("pyLR1")
    logger.error("{}:{}:{}: {}: {}".format(pos.file, pos.line0,
                                           pos.col0, "error", errmsg))

def warning(parser, pos, warning):
    logger = logging.getLogger("pyLR1")
    logger.warning("{}:{}:{}: {}: {}".format(pos.file, pos.line0,
                                             pos.col0, "warning", errmsg))


def indent(lexer, text, position):
    if not hasattr(lexer, 'indent_stack'):
        lexer.indent_stack = [0]

    if len(text) > lexer.indent_stack[-1]:
        lexer.indent_stack.append(len(text))
        lexer.next_cond.pop()
        lexer.push_back((lexer.TOKEN_NUMBERS['INDENT'], text, position))
        return

    # pop the state pushed on INDENT
    lexer.next_cond.pop()

    cnt = 0
    try:
        while lexer.indent_stack.pop() != len(text):
            cnt += 1
    except IndexError:
        return (lexer.TOKEN_NUMBERS['$ERROR'], text, position)

    lexer.indent_stack.append(len(text))

    for i in range(cnt):
        # pop one further state per DEDENT
        lexer.next_cond.pop()
        lexer.push_back((lexer.TOKEN_NUMBERS['DEDENT'], text, position))

    lexer.next_cond.pop()

class AnnotatedRuleset(object):

    def __init__(self, symbol, pos, altname=None):
        self.symbol = symbol
        self.position = pos
        self.altname = altname

    def add_prod(self, prod):
        # unwrap the payload and apply
        self.symbol.add_prod(prod.production)

        # run the varmapper
        if prod.action is not None:
            prod.name_result(self.symbol.name, self.position)
            if self.altname is not None:
                prod.name_result(self.altname, self.position, explicit=True)

            varmapper = PyBlobStackVarMapVisitor(prod.varmap)
            varmapper.visit(prod.action)

class AnnotatedProduction(object):

    def __init__(self, parser, production):
        """
        A `Production` annotated with names for the single elements.
        """
        self.production = production
        self.names = {}
        self.ambiguous = set()
        self.parser = parser

    @property
    def action(self):
        return self.production.action

    @action.setter
    def action(self, value):
        self.production.action = value

    def varmap(self, var):
        """
        A varmap function for the PyBlobStackVarMapVisitor
        """
        text = var.text[1:]

        if text == '$':
            return None

        try:
            num = int(text)

            if num > len(self.production):
                error(self.parser, var.position,
                      "invalid stack var ${}: value too large".format(num))
        except ValueError:
            try:
                if text in self.ambiguous:
                    error(self.parser, var.position,
                          "ambiguous stack var ${}".format(text))

                num = self.names[text]
            except KeyError:
                error(self.parser, var.position,
                      "undefined stack var ${}".format(text))
                return -1

        return num

    def _name_check(self, name, value, explicit, collision_msg, pos):
        if name in self.names:
            if explicit:
                error(self.parser, pos, collision_msg)
            else:
                self.ambiguous.add(name)

        self.names[name] = value

    def name_result(self, name, pos, explicit=False):
        err_msg = "explicite result var name already taken"
        self._name_check(name, None, explicit, err_msg, pos)

    def name_last(self, name, pos, explicit=False):
        err_msg = "explicitly redefined symbol name"
        self._name_check(name, len(self.production), explicit, err_msg, pos)

class PyBlobStackVarMapVisitor(PyBlobVisitor):

    def __init__(self, varmap):
        self.varmap = varmap

    def visit_PySuite(self, suite):
        for fragment in suite.code:
            fragment.accept(self)

    def visit_PyText(self, text):
        pass

    def visit_PyNewline(self, newline):
        pass

    def visit_PyStackvar(self, stackvar):
        num = self.varmap(stackvar)
        if num is None:
            stackvar.result = True
        else:
            stackvar.num = num


def bind_named_pattern(self, name, regex):
    try:
        self.syntax.lexer.add_named_pattern(
            name.sem,
            Regex(regex.sem, bindings=self.syntax.lexer.named_patterns(),
                  ucd=self._ucd).ast
        )
    except (RegexSyntaxError, SyntaxNameError) as e:
        error(self, regex.pos,
              "syntax error in regex def: {}".format(e.args[0]))


def add_lexrule(self, start_conditions, regex, lexaction):
    try:
        regex = Regex(regex.sem, bindings=self.syntax.lexer.named_patterns(),
                      ucd=self._ucd)
    except (RegexSyntaxError, SyntaxNameError) as e:
        error(self, regex.pos,
              "syntax error in regex: {}".format(e.args[0]))
    lex_rule = LexingRule(start_conditions, regex, lexaction.sem)
    self.syntax.lexer.add_lexing_rule(lex_rule)

def get_initial_condition(self, pos, name):
    try:
        return self.syntax.lexer.initial_condition(name)
    except SyntaxNameError as e:
        error(self, pos, str(e))
        self.syntax.lexer.add_inclusive_initial_condition(name)
        return self.syntax.lexer.initial_condition(name)

def setup_production(self):
    prod = Production(None, [], self.productionNumber)
    self.productionNumber += 1
    return AnnotatedProduction(self, prod)

def set_assoc(parser, assoctype, symbols):
    assoc = assoctype, parser.assocPower
    for symb in symbols:
        parser.assocDefs[symb] = assoc
    parser.assocPower += 1

def add_meta_symbol(self, prod, symbol, pos, altname=None):
    sym = self.syntax.symtable.require_symbol(symbol, pos)

    prod.production.assoc = self.assocDefs.get(sym.name,
                                               prod.production.assoc)
    prod.production.add_sym(sym)

    prod.name_last(symbol, pos)
    if altname is not None:
        prod.name_last(altname, pos, explicit=True)

def add_stoken(self, prod, stoken, pos):
    sym = self.syntax.define_s_token(stoken)
    prod.production.assoc = self.assocDefs.get(sym.name,
                                               prod.production.assoc)
    prod.production.add_sym(sym)

def add_error(self, prod, pos):
    sym = self.syntax.symtable.require_recover()
    prod.production.add_sym(sym)

def check_for_undefined_metas(parser):
    for symbol, posis in sorted(parser.syntax.symtable.undef(),
                                key=lambda x: x[0].name):
        for pos in posis:
            error(parser, pos,
                  "used undefined symbol {}".format(symbol.name))

class PyBlobBuilder(object):
    def __init__(self):
        self.blob = PySuite()

    def get(self):
        return self.blob

    def newline(self):
        self.blob.add(PyNewline())

    def text(self, text):
        self.blob.add(PyText(text))

    def stackvar(self, sv, pos):
        self.blob.add(PyStackvar(text=sv, position=pos))

    def suite(self, suite):
        self.blob.add(suite)


# for debugging purposes
if __name__ == '__main__':
    import sys
    l = Lexer(open(sys.argv[2], 'r'))

    if sys.argv[1] == '-l':
        token_type = -1
        while token_type != Lexer.TOKEN_NUMBERS['$EOF']:
            token_type, lexeme, position = l.lex()
            print(Lexer.TOKEN_NAMES[token_type],
                  '"""{}"""'.format(lexeme), l.nextCond)
    elif sys.argv[1] == '-p':
        p = Parser(l)
        try:
            p.Parse()
        except SyntaxError as e:
            print(str(e))
